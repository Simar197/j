{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmB0x09R0Sgh"
      },
      "source": [
        "## Stage I : SeamFormer\n",
        "\n",
        "Goal : The purpose of this notebook is for users to perform quick inference on their custom datasets without finetuning/training. We offer multiple model checkpoints for you to try out !\n",
        "\n",
        "Inputs to the notebook :  \n",
        "\n",
        "* Image Folder\n",
        "\n",
        "Pointers on how to choose the optimal model weight :\n",
        "\n",
        "- If your custom input images are more like Palm Leaf Manuscripts ( Balineese/Sundaneese/Khmer ) , you can opt for the checkpoint : BKS.pt\n",
        "\n",
        "- If your custom input images are mode dense in nature and are closely related to Indic documents , you may experiment with : I2.pt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f53O1PAH5J8j"
      },
      "source": [
        "# Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9w8u8UFyKnk",
        "outputId": "afffff4b-a5e1-49c3-db5b-fa9b3a86af9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vit_pytorch==0.24.3 in /usr/local/lib/python3.10/dist-packages (0.24.3)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.10/dist-packages (from vit_pytorch==0.24.3) (0.6.1)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from vit_pytorch==0.24.3) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from vit_pytorch==0.24.3) (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->vit_pytorch==0.24.3) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->vit_pytorch==0.24.3) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->vit_pytorch==0.24.3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->vit_pytorch==0.24.3) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->vit_pytorch==0.24.3) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->vit_pytorch==0.24.3) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->vit_pytorch==0.24.3) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->vit_pytorch==0.24.3) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->vit_pytorch==0.24.3) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->vit_pytorch==0.24.3) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->vit_pytorch==0.24.3) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->vit_pytorch==0.24.3) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->vit_pytorch==0.24.3) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->vit_pytorch==0.24.3) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->vit_pytorch==0.24.3) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->vit_pytorch==0.24.3) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->vit_pytorch==0.24.3) (1.3.0)\n",
            "Requirement already satisfied: empatches in /usr/local/lib/python3.10/dist-packages (0.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from empatches) (1.22.4)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: plantcv==3.14.1 in /usr/local/lib/python3.10/dist-packages (3.14.1)\n",
            "Requirement already satisfied: matplotlib>=1.5 in /usr/local/lib/python3.10/dist-packages (from plantcv==3.14.1) (3.7.1)\n",
            "Requirement already satisfied: numpy<1.23,>=1.11 in /usr/local/lib/python3.10/dist-packages (from plantcv==3.14.1) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from plantcv==3.14.1) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from plantcv==3.14.1) (2.8.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from plantcv==3.14.1) (1.10.1)\n",
            "Requirement already satisfied: scikit-image>=0.13 in /usr/local/lib/python3.10/dist-packages (from plantcv==3.14.1) (0.19.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from plantcv==3.14.1) (1.2.2)\n",
            "Requirement already satisfied: plotnine in /usr/local/lib/python3.10/dist-packages (from plantcv==3.14.1) (0.10.1)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from plantcv==3.14.1) (2023.8.0)\n",
            "Requirement already satisfied: dask-jobqueue in /usr/local/lib/python3.10/dist-packages (from plantcv==3.14.1) (0.8.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from plantcv==3.14.1) (4.8.0.76)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from plantcv==3.14.1) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->plantcv==3.14.1) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->plantcv==3.14.1) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->plantcv==3.14.1) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->plantcv==3.14.1) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->plantcv==3.14.1) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->plantcv==3.14.1) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->plantcv==3.14.1) (3.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->plantcv==3.14.1) (1.16.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.13->plantcv==3.14.1) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.13->plantcv==3.14.1) (2.31.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.13->plantcv==3.14.1) (2023.8.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.13->plantcv==3.14.1) (1.4.1)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask->plantcv==3.14.1) (8.1.6)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask->plantcv==3.14.1) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask->plantcv==3.14.1) (2023.6.0)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask->plantcv==3.14.1) (1.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask->plantcv==3.14.1) (6.0.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask->plantcv==3.14.1) (0.12.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask->plantcv==3.14.1) (6.8.0)\n",
            "Requirement already satisfied: distributed>=2022.02.0 in /usr/local/lib/python3.10/dist-packages (from dask-jobqueue->plantcv==3.14.1) (2023.8.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->plantcv==3.14.1) (2023.3)\n",
            "Requirement already satisfied: mizani>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from plotnine->plantcv==3.14.1) (0.9.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from plotnine->plantcv==3.14.1) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->plantcv==3.14.1) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->plantcv==3.14.1) (3.2.0)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask-jobqueue->plantcv==3.14.1) (3.1.2)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask-jobqueue->plantcv==3.14.1) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask-jobqueue->plantcv==3.14.1) (1.0.5)\n",
            "Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask-jobqueue->plantcv==3.14.1) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask-jobqueue->plantcv==3.14.1) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask-jobqueue->plantcv==3.14.1) (2.0.0)\n",
            "Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask-jobqueue->plantcv==3.14.1) (6.3.2)\n",
            "Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask-jobqueue->plantcv==3.14.1) (2.0.4)\n",
            "Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask-jobqueue->plantcv==3.14.1) (3.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask->plantcv==3.14.1) (3.16.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed>=2022.02.0->dask-jobqueue->plantcv==3.14.1) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "# Requires installations\n",
        "!pip install vit_pytorch==0.24.3\n",
        "!pip install empatches\n",
        "!pip install gdown\n",
        "!pip install plantcv==3.14.1\n",
        "\n",
        "# Library Imports\n",
        "import copy\n",
        "import cv2\n",
        "import os\n",
        "import pathlib\n",
        "import sys\n",
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "from vit_pytorch import ViT\n",
        "from einops import rearrange\n",
        "from empatches import EMPatches\n",
        "from plantcv import plantcv as pcv\n",
        "pcv.params.debug = None\n",
        "\n",
        "\n",
        "# Torch Imports\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from vit_pytorch.vit import Transformer\n",
        "from google.colab.patches import cv2_imshow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs7dKEfH7Zdq"
      },
      "source": [
        "# Global Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNMRiLs57cBt"
      },
      "outputs": [],
      "source": [
        "# Default Inputs\n",
        "# THRESHOLD = 0.3 ## binarization threshold after the model output\n",
        "# SPLITSIZE =  256  ## your image will be divided into patches of 256x256 pixels\n",
        "# setting = \"base\"  ## choose the desired model size [small, base or large], depending on the model you want to use\n",
        "# patch_size = 8 ## choose your desired patch size [8 or 16], depending on the model you want to use\n",
        "# image_size =  (SPLITSIZE,SPLITSIZE)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "settings={\n",
        "    \"dataset_code\":\"BKS\",\n",
        "    \"experiment_base\":\"scribbleGen\",\n",
        "    \"wid\" : \"BKS_L2_Loss_V3_EncFreeze_MorePatches_th7\",\n",
        "    \"data_path\":\"/scratch/nv/BKS/\",\n",
        "    \"model_weights_path\":\"/scratch/nv/weights_BKS_Exp/\",\n",
        "    \"visualisation_folder\":\"/scratch/nv/vis_BKS_Exp/\",\n",
        "    \"learning_rate\":0.005,\n",
        "    \"vit_model_size\":\"base\",\n",
        "    \"imgsize\":256,\n",
        "    \"patchsize\":8,\n",
        "    \"split_size\":256,\n",
        "    \"vit_patch_size\":8,\n",
        "    \"encoder_freeze\":\"False\",\n",
        "    \"encoder_layers\":6,\n",
        "    \"encoder_heads\":8,\n",
        "    \"encoder_dims\":768,\n",
        "    \"batch_size\":8,\n",
        "    \"num_epochs\":25,\n",
        "    \"train_scribble\":False,\n",
        "    \"train_binary\":False,\n",
        "    \"vis_results\":\"True\",\n",
        "    \"scribble_weights_path\":\"/content/drive/MyDrive/Stable_SF/network-Public_BKS_E3_V3-9.pt\",\n",
        "    \"binary_weights_path\":'/content/drive/MyDrive/Stable_SF/BKS_correct_final.pt',\n",
        "    \"threshold\":0.3\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpCjSmhjh0VG"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJykXI7ghzjS"
      },
      "outputs": [],
      "source": [
        "def downloadWeights(modelType):\n",
        "  if modelType =='I2':\n",
        "    if not os.path.exists('I2.pt') :\n",
        "      !gdown 1O_CtJToNUPrQzbMN38FsOJwEdxCDXqHh\n",
        "    else:\n",
        "      print('I2.pt is already existing .. Skipping download !')\n",
        "  elif modelType == 'BKS':\n",
        "    if not os.path.exists('BKS.pt') :\n",
        "      !gdown 1nro1UjYRSlMIaYUwkMTrfZzrE_kz0QDF\n",
        "    else:\n",
        "      print('BKS.pt is already existing .. Skipping download !')\n",
        "  else :\n",
        "    print('Invalid Model Checkpoint')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqwDkljjiGu7"
      },
      "source": [
        "# Network Instantiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yYwxhe2iFru"
      },
      "outputs": [],
      "source": [
        "class SeamFormer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        encoder,\n",
        "        decoder_dim,\n",
        "        decoder_depth = 1,\n",
        "        decoder_heads = 8,\n",
        "        decoder_dim_head = 64,\n",
        "        patch_size =8):\n",
        "\n",
        "        super().__init__()\n",
        "        # extract hyperparameters and functions from the ViT encoder.\n",
        "        self.encoder = encoder\n",
        "        num_patches, encoder_dim = encoder.pos_embedding.shape[-2:]\n",
        "        self.to_patch, self.patch_to_emb = encoder.to_patch_embedding[:2]\n",
        "        # pixel_values_per_patch = self.patch_to_emb.weight.shape[-1]\n",
        "        pixel_values_per_patch = patch_size * patch_size\n",
        "\n",
        "        # Binary Decoder\n",
        "        self.enc_to_dec_bin = nn.Linear(encoder_dim, decoder_dim) if encoder_dim != decoder_dim else nn.Identity()\n",
        "        self.mask_token_bin = nn.Parameter(torch.randn(decoder_dim))\n",
        "        self.decoder_bin = Transformer(dim = decoder_dim, depth = decoder_depth, heads = decoder_heads, dim_head = decoder_dim_head, mlp_dim = decoder_dim * 4)\n",
        "        self.decoder_pos_emb_bin = nn.Embedding(num_patches, decoder_dim)\n",
        "        self.to_pixels_bin = nn.Linear(decoder_dim, pixel_values_per_patch)\n",
        "\n",
        "        # Scribble Decoder\n",
        "        self.enc_to_dec_scr = nn.Linear(encoder_dim, decoder_dim) if encoder_dim != decoder_dim else nn.Identity()\n",
        "        self.mask_token_scr = nn.Parameter(torch.randn(decoder_dim))\n",
        "        self.decoder_scr = Transformer(dim = decoder_dim, depth = decoder_depth, heads = decoder_heads, dim_head = decoder_dim_head, mlp_dim = decoder_dim * 4)\n",
        "        self.decoder_pos_emb_scr = nn.Embedding(num_patches, decoder_dim)\n",
        "        self.to_pixels_scr = nn.Linear(decoder_dim, pixel_values_per_patch)\n",
        "\n",
        "\n",
        "    def forward(self,img,gt_bin_img=None,gt_scr_img=None,criterion=None,strain=True,btrain=True,mode='train'):\n",
        "        scribbleloss=None\n",
        "        gt_scr_patches=None\n",
        "        binaryloss=None\n",
        "        gt_bin_patches=None\n",
        "\n",
        "        # get patches and their number\n",
        "        patches = self.to_patch(img)\n",
        "        _, num_patches, *_ = patches.shape\n",
        "        # project pixel patches to tokens and add positions\n",
        "        tokens = self.patch_to_emb(patches)\n",
        "        tokens = tokens + self.encoder.pos_embedding[:, 1:(num_patches + 1)]\n",
        "        # encode tokens by the encoder\n",
        "        encoded_tokens = self.encoder.transformer(tokens)\n",
        "\n",
        "        if btrain:\n",
        "            decoder_tokens_bin = self.enc_to_dec_bin(encoded_tokens)\n",
        "            # decode tokens with decoder\n",
        "            decoded_tokens_bin = self.decoder_bin(decoder_tokens_bin)\n",
        "            # project tokens to pixels\n",
        "            pred_pixel_values_bin = self.to_pixels_bin(decoded_tokens_bin)\n",
        "            ## --- Focal Loss ---\n",
        "            if mode == 'train':\n",
        "                # calculate the loss with gt\n",
        "                if gt_bin_img is not None:\n",
        "                    gt_bin_patches = self.to_patch(gt_bin_img)\n",
        "                ## ---  Weighted BCE Loss ---\n",
        "                binaryloss = criterion(pred_pixel_values_bin,gt_bin_patches)\n",
        "                # pt = torch.exp(-binaryloss)\n",
        "                # binaryloss = ((1-pt)**2) * binaryloss\n",
        "                # binaryloss = torch.mean(binaryloss)\n",
        "                return binaryloss,gt_bin_patches,pred_pixel_values_bin\n",
        "\n",
        "        if strain:\n",
        "            decoder_tokens_scr = self.enc_to_dec_scr(encoded_tokens)\n",
        "            # decode tokens with decoder\n",
        "            decoded_tokens_scr = self.decoder_scr(decoder_tokens_scr)\n",
        "            # project tokens to pixels\n",
        "            pred_pixel_values_scr = self.to_pixels_scr(decoded_tokens_scr)\n",
        "            ## --- Focal Loss ---\n",
        "            if mode == 'train':\n",
        "                # calculate the loss with gt\n",
        "                if gt_scr_img is not None:\n",
        "                    gt_scr_patches = self.to_patch(gt_scr_img)\n",
        "                ## ---  Weighted BCE Loss ---\n",
        "                scribbleloss = criterion(pred_pixel_values_scr,gt_scr_patches)\n",
        "                # pt = torch.exp(-scribbleloss)\n",
        "                # scribbleloss = ((1-pt)**2) *scribbleloss\n",
        "                # scribbleloss = torch.mean(scribbleloss)\n",
        "                return scribbleloss,gt_scr_patches,pred_pixel_values_scr\n",
        "\n",
        "        if mode=='test':\n",
        "            return pred_pixel_values_bin,pred_pixel_values_scr\n",
        "\n",
        "\n",
        "\n",
        "# Network Instantiation\n",
        "\n",
        "# Encoder settings\n",
        "encoder_layers = settings['encoder_layers']\n",
        "encoder_heads = settings['encoder_heads']\n",
        "encoder_dim = settings['encoder_dims']\n",
        "\n",
        "# Encoder\n",
        "v = ViT(\n",
        "    image_size = settings['imgsize'],\n",
        "    patch_size =  settings['patchsize'],\n",
        "    num_classes = 1000,\n",
        "    dim = encoder_dim,\n",
        "    depth = encoder_layers,\n",
        "    heads = encoder_heads,\n",
        "    mlp_dim = 2048)\n",
        "\n",
        "# Full model\n",
        "network = SeamFormer(encoder = v,\n",
        "    decoder_dim = encoder_dim,\n",
        "    decoder_depth = encoder_layers,\n",
        "    decoder_heads = encoder_heads).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAYQ4OqElCdg"
      },
      "source": [
        "# Helper Function : Reading Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBQwNkbylLTV"
      },
      "outputs": [],
      "source": [
        "def preprocess(deg_img):\n",
        "    deg_img = (np.array(deg_img) /255).astype('float32')\n",
        "    # normalize data\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "    out_deg_img = np.zeros([3, *deg_img.shape[:-1]])\n",
        "    for i in range(3):\n",
        "        out_deg_img[i] = (deg_img[:,:,i] - mean[i]) / std[i]\n",
        "    return out_deg_img\n",
        "\n",
        "def deformat(listofpoints):\n",
        "    # Input : [[[x1,y1],[[x2,y2]],[[x3,y3]]....]\n",
        "    # Output : [ [x1,y1], [x2,y2],[x3,y3]....]\n",
        "    output = [ pt[0].tolist() for pt in listofpoints ]\n",
        "    return output\n",
        "\n",
        "# Supply the raw image here\n",
        "def cleanImageFindContours(patch,threshold = 0.20):\n",
        "    patch = np.uint8(patch)\n",
        "    #   patch = cv2.cvtColor(patch,cv2.COLOR_BGR2GRAY)\n",
        "    contours, hierarchy = cv2.findContours(patch,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "    if len(contours)<1:\n",
        "        print('No contours in the raw image!')\n",
        "        return patch\n",
        "    # Else sort them\n",
        "    cntsSorted = sorted(contours, key=lambda x: cv2.contourArea(x),reverse=True)\n",
        "    areaList = [cv2.contourArea(c) for c in cntsSorted]\n",
        "    maxArea = max(areaList)\n",
        "    sortedContours = [deformat(c) for c in cntsSorted if cv2.contourArea(c)>threshold*maxArea]\n",
        "    # Draw on canvas\n",
        "    canvas=np.zeros(patch.shape)\n",
        "    for i,cnt in enumerate(sortedContours):\n",
        "        canvas= cv2.fillPoly(canvas,np.int32([cnt]),color=(255,255,255))\n",
        "    return sortedContours,canvas\n",
        "\n",
        "\n",
        "\n",
        "def readFullImage(path,PDIM=256,DIM=256,OVERLAP=0.25):\n",
        "    input_patches=[]\n",
        "    emp = EMPatches()\n",
        "    try:\n",
        "        img = cv2.imread(path)\n",
        "        img = preprocess(img)\n",
        "        img = np.transpose(img)\n",
        "        img_patches, indices = emp.extract_patches(img,patchsize=PDIM,overlap=OVERLAP)\n",
        "        for i,patch in enumerate(img_patches):\n",
        "              resized=[DIM,DIM]\n",
        "              if patch.shape[0]!= DIM or patch.shape[1]!= DIM :\n",
        "                  resized=[patch.shape[0],patch.shape[1]]\n",
        "                  patch = cv2.resize(patch,(DIM,DIM),interpolation = cv2.INTER_AREA)\n",
        "              # cv2_imshow(patch)\n",
        "              patch = np.asarray(patch,dtype=np.float32)\n",
        "              patch =  np.transpose(patch)\n",
        "              patch= np.expand_dims(patch,axis=0)\n",
        "              sample={'img':patch,'resized':resized}\n",
        "              input_patches.append(sample)\n",
        "\n",
        "    except Exception as exp :\n",
        "        print('ImageReading Error ! :{}'.format(exp))\n",
        "        return None\n",
        "    return input_patches,indices\n",
        "\n",
        "def stack_images_vertically(image_a, image_b,image_c):\n",
        "    \"\"\"\n",
        "    Stacks two images vertically on top of each other.\n",
        "\n",
        "    Args:\n",
        "        image_a: The first image (numpy array).\n",
        "        image_b: The second image (numpy array).\n",
        "\n",
        "    Returns:\n",
        "        The vertically stacked image (numpy array).\n",
        "\n",
        "    Raises:\n",
        "\n",
        "        ValueError: If the images have different widths.\n",
        "    \"\"\"\n",
        "    # Check if the dimensions of the two images match\n",
        "    if image_a.shape[1] != image_b.shape[1] or image_a.shape[1] != image_c.shape[1]:\n",
        "        raise ValueError(\"The images must have the same width\")\n",
        "\n",
        "    # Stack the images vertically\n",
        "    stacked_image = np.vstack((image_a, image_b,image_c))\n",
        "    return stacked_image\n",
        "\n",
        "\n",
        "def reconstruct(pred_pixel_values,patch_size,target_shape,image_size):\n",
        "    rec_patches = copy.deepcopy(pred_pixel_values)\n",
        "    output_image = rearrange(rec_patches, 'b (h w) (p1 p2 c) -> b c (h p1) (w p2)',p1 = patch_size, p2 = patch_size,  h=image_size[0]//patch_size)\n",
        "    output_image = output_image.cpu().numpy().squeeze()\n",
        "    output_image =  output_image.T\n",
        "    # Resizing to get desired output\n",
        "    output_image = cv2.resize(output_image,target_shape, interpolation = cv2.INTER_AREA)\n",
        "    # Basic Thresholding\n",
        "    output_image[np.where( output_image>1)] = 1\n",
        "    output_image[np.where( output_image<0)] = 0\n",
        "    return output_image\n",
        "\n",
        "\n",
        "def inferenceNetwork(network,path,PDIM=256,DIM=256,OVERLAP=0.25,THRESHOLD=0.3,save=True):\n",
        "    parentImage=cv2.imread(path)\n",
        "    patch_size = 8\n",
        "    emp = EMPatches()\n",
        "    if not os.path.exists(path):\n",
        "        print('Invalid File Path ! Skipping Inference')\n",
        "        sys.exit()\n",
        "    else:\n",
        "        input_patches , indices = readFullImage(path,PDIM,DIM,OVERLAP)\n",
        "        soutput_patches=[]\n",
        "        boutput_patches=[]\n",
        "        # Iterate through the resulting patches\n",
        "        weight = torch.tensor(1)\n",
        "        for i,sample in enumerate(input_patches):\n",
        "            p = sample['img']\n",
        "            target_shape = (sample['resized'][1],sample['resized'][0])\n",
        "            with torch.no_grad():\n",
        "                inputs =torch.from_numpy(p).to(device)\n",
        "                # Pass through model\n",
        "                # loss, patches, pred_pixel_values = model(inputs,inputs,criterion,train=False)\n",
        "                loss_criterion = torch.nn.BCEWithLogitsLoss(pos_weight=weight, reduction='none')\n",
        "                pred_pixel_values_bin,pred_pixel_values_scr=network(inputs,gt_bin_img=inputs,gt_scr_img=inputs,criterion=loss_criterion,strain=True,btrain=True,mode='test')\n",
        "\n",
        "                # Send them to .cpu\n",
        "                pred_pixel_values_bin = pred_pixel_values_bin.cpu()\n",
        "                pred_pixel_values_scr = pred_pixel_values_scr.cpu()\n",
        "\n",
        "                bpatch=reconstruct(pred_pixel_values_bin,patch_size,target_shape,(DIM,DIM))\n",
        "                spatch=reconstruct(pred_pixel_values_scr,patch_size,target_shape,(DIM,DIM))\n",
        "\n",
        "                # binarize the predicted image taking 0.5 as threshold\n",
        "                bpatch = ( bpatch>THRESHOLD)*1\n",
        "                spatch = ( spatch>THRESHOLD)*1\n",
        "\n",
        "                # Append the net processed patch\n",
        "                soutput_patches.append(255*spatch)\n",
        "                boutput_patches.append(255*bpatch)\n",
        "\n",
        "        assert len(boutput_patches)==len(soutput_patches)==len(input_patches),\"Error in patch count!\"\n",
        "\n",
        "        # Restich the image\n",
        "        soutput = emp.merge_patches(soutput_patches,indices,mode='max')\n",
        "        boutput = emp.merge_patches(boutput_patches,indices,mode='max')\n",
        "\n",
        "        # Binary Done\n",
        "        binaryOutput=np.transpose(boutput,(1,0))\n",
        "\n",
        "        # Scribble Done\n",
        "        soutput=np.transpose(soutput,(1,0))\n",
        "        contours,scribbleOutput=cleanImageFindContours(patch=soutput.astype(np.uint8),threshold = 0.15)\n",
        "\n",
        "        # Sharing both\n",
        "        res= stack_images_vertically(image_a=np.asarray(parentImage[:,:,0],dtype=np.uint8).squeeze(),image_b=scribbleOutput,image_c=binaryOutput)\n",
        "        return binaryOutput,scribbleOutput,res\n",
        "\n",
        "# Utils\n",
        "def imageCombiner(imgs):\n",
        "  imgs_comb = np.hstack([i for i in imgs])\n",
        "  return imgs_comb\n",
        "\n",
        "\n",
        "# Polygon to Distance Mask\n",
        "def polygon_to_distance_mask(polygon_mask,threshold=60):\n",
        "    # Read the polygon mask image as a binary image\n",
        "    # polygon_mask = cv2.cvtColor(pmask,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Ensure that the mask is binary (0 or 255 values)\n",
        "    _, polygon_mask = cv2.threshold(polygon_mask,100,255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Compute the distance transform\n",
        "    distance_mask = cv2.distanceTransform(polygon_mask, cv2.DIST_L2, cv2.DIST_MASK_5)\n",
        "\n",
        "    # Normalize the distance values to 0-255 range\n",
        "    distance_mask = cv2.normalize(distance_mask, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "\n",
        "    # Threshold the image\n",
        "    src = copy.deepcopy(distance_mask)\n",
        "    src[src<threshold]=0\n",
        "    src[src>=threshold]=255\n",
        "    src = np.uint8(src)\n",
        "    return src\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU1fOiMRr6MA"
      },
      "source": [
        "# Helper Function : Post Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX3YsJIGr-aT"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Post Processing Function\n",
        "'''\n",
        "\n",
        "def find_corner_points(points):\n",
        "    if not points:\n",
        "        return None, None\n",
        "    # Sort the points based on their x-coordinate\n",
        "    sorted_points = sorted(points, key=lambda point: point[0])\n",
        "    leftmost_point = list(sorted_points[0])\n",
        "    rightmost_point = list(sorted_points[-1])\n",
        "    return leftmost_point, rightmost_point\n",
        "\n",
        "def hullNise(polygons):\n",
        "  hulls = []\n",
        "  if len(polygons)==0:\n",
        "    return hulls\n",
        "  else:\n",
        "    for p in polygons:\n",
        "      p = np.asarray(p,dtype=np.int32)\n",
        "      hull = cv2.convexHull(p)\n",
        "      hull =  np.asarray(hull,dtype=np.int32).reshape((-1,2))\n",
        "      hulls.append(hull.tolist())\n",
        "  return hulls\n",
        "\n",
        "def uniformly_sampled_line(points,T=50):\n",
        "    num_points = min(len(points),T)\n",
        "    # Separate x and y coordinates from the given points\n",
        "    x_coords, y_coords = zip(*points)\n",
        "\n",
        "    # Calculate the cumulative distance along the original line\n",
        "    distances = np.cumsum(np.sqrt(np.diff(x_coords) ** 2 + np.diff(y_coords) ** 2))\n",
        "    distances = np.insert(distances, 0, 0)  # Add the initial point (0, 0) distance\n",
        "\n",
        "    # Create a linear interpolation function for x and y coordinates\n",
        "    interpolate_x = interp1d(distances, x_coords, kind='linear')\n",
        "    interpolate_y = interp1d(distances, y_coords, kind='linear')\n",
        "\n",
        "    # Calculate new uniformly spaced distances\n",
        "    new_distances = np.linspace(0, distances[-1], num_points)\n",
        "\n",
        "    # Interpolate new x and y coordinates using the uniformly spaced distances\n",
        "    new_x_coords = interpolate_x(new_distances)\n",
        "    new_y_coords = interpolate_y(new_distances)\n",
        "\n",
        "    # Create a list of new points\n",
        "    new_points = [[np.int32(new_x_coords[i]), np.int32(new_y_coords[i])] for i in range(num_points)]\n",
        "    return new_points\n",
        "\n",
        "# Scribble Generation\n",
        "def generateScribble(H,W,polygon):\n",
        "    # Generate Canvas\n",
        "    canvas = np.zeros((H,W))\n",
        "    # Mark the polygon on the canvas\n",
        "    leftmost_point, rightmost_point = find_corner_points(polygon)\n",
        "    poly_arr = np.asarray(polygon,dtype=np.int32).reshape((-1,1,2))\n",
        "    canvas = cv2.fillPoly(canvas,[poly_arr],(255,255,255))\n",
        "    # Scribble generation\n",
        "    skeleton = pcv.morphology.skeletonize(canvas)\n",
        "    pruned_skeleton,_,segment_objects = pcv.morphology.prune(skel_img=skeleton,size=100)\n",
        "    scribble = np.asarray(segment_objects[0],dtype=np.int32).reshape((-1,2))\n",
        "    scribble=scribble.tolist()\n",
        "    # scribble = collect_mask_points(pruned_skeleton)\n",
        "    scribble = uniformly_sampled_line(scribble,1000)\n",
        "    if leftmost_point is not None and rightmost_point is not None:\n",
        "      scribble.append(leftmost_point)\n",
        "      scribble.append(rightmost_point)\n",
        "      scribble = sorted(scribble, key=lambda point: point[0])\n",
        "    return scribble\n",
        "\n",
        "# Text Dilation\n",
        "def text_dilate(image, kernel_size, iterations=1):\n",
        "    # Create a structuring element (kernel) for dilation\n",
        "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "    # Perform dilation\n",
        "    dilated_image = cv2.dilate(image, kernel, iterations=iterations)\n",
        "    return dilated_image\n",
        "\n",
        "# Horizontal Dilation\n",
        "def horizontal_dilation(image, kernel_width=5,iterations=1):\n",
        "    # Create a horizontal kernel for dilation\n",
        "    kernel = np.ones((1, kernel_width), np.uint8)\n",
        "    # Perform dilation\n",
        "    dilated_image = cv2.dilate(image, kernel, iterations)\n",
        "    return dilated_image\n",
        "\n",
        "\n",
        "def average_coordinates(hull):\n",
        "    # Calculate the average x and y coordinates of all points in the hull/contour.\n",
        "    # Format : [[x1,y1], [x2,y2],[x3,y3]...[xn,yn]]\n",
        "    num_points = len(hull)\n",
        "    avg_x = sum(pt[0][0] for pt in hull) / num_points\n",
        "    avg_y = sum(pt[0][1] for pt in hull) / num_points\n",
        "    return avg_x, avg_y\n",
        "\n",
        "# You send set of clean contours to this function\n",
        "# you obtain a list of hulls and merge the ones on the same horizontal level.\n",
        "def combine_hulls_on_same_level(contours, threshold=45):\n",
        "    combined_hulls=[]\n",
        "    hulls = hullNise(contours)\n",
        "\n",
        "    # Sort the hulls by the average y-coordinate of all points\n",
        "    sorted_hulls = sorted(hulls, key=lambda hull: average_coordinates(hull)[1])\n",
        "\n",
        "    current_combined_hull = sorted_hulls[0]\n",
        "    for hull in sorted_hulls[1:]:\n",
        "        # Check if the current hull is on the same horizontal level as the combined hull\n",
        "        if abs(average_coordinates(hull)[1] - average_coordinates(current_combined_hull)[1]) < threshold:\n",
        "            # Merge the hulls by extending the current_combined_hull with hull\n",
        "            current_combined_hull = np.vstack((current_combined_hull, hull))\n",
        "        else:\n",
        "            # Hull is on a different level, add the current combined hull to the result\n",
        "            combined_hulls.append(current_combined_hull)\n",
        "            current_combined_hull = hull\n",
        "\n",
        "    # Add the last combined hull\n",
        "    combined_hulls.append(current_combined_hull)\n",
        "    nethulls = [cv2.convexHull(np.array(contour)) for contour in combined_hulls]\n",
        "    return nethulls\n",
        "\n",
        "def postProcess(scribbleImage,binaryImage,binaryThreshold=50,rectangularKernel=50):\n",
        "    bin_ = binaryImage.astype(np.uint8)\n",
        "    scr = scribbleImage.astype(np.uint8)\n",
        "    # print('PP @ BIN SHAPE : {} SCRIBBLE SHAPE : {}'.format(scribbleImage.shape,binaryImage.shape))\n",
        "    # bin_ = cv2.cvtColor(bin_,cv2.COLOR_BGR2GRAY)\n",
        "    H,W = bin_.shape\n",
        "\n",
        "    # Threshold it\n",
        "    bin_[bin_>=binaryThreshold]=255\n",
        "    bin_[bin_<binaryThreshold]=0\n",
        "    scr[scr>=binaryThreshold]=255\n",
        "    scr[scr<binaryThreshold]=0\n",
        "\n",
        "    # We apply distance transform to thin the output polygon\n",
        "    scr = polygon_to_distance_mask(scr,threshold=50)\n",
        "\n",
        "    # Bitwise AND of the textual region and polygon region ( only cut off letters will be highlighted)\n",
        "    scr_ = cv2.bitwise_and(bin_/255,scr/255)\n",
        "    # Dilate the existing text content\n",
        "    scr_ = text_dilate(scr_,kernel_size=3,iterations=3) # SD = 3,3\n",
        "    # Dilate it horizontally to fill the gaps within the text region\n",
        "    scr_ = horizontal_dilation(scr_,rectangularKernel,3) # SD - 50 ,3\n",
        "\n",
        "    # Extract the final contours\n",
        "    contours = cleanImageFindContours(np.uint8(scr_),threshold = 0.10)\n",
        "\n",
        "    print('Length of contours : {}'.format(len(contours)))\n",
        "    print(contours[0])\n",
        "\n",
        "    # Combine the hulls that are on the same horizontal level\n",
        "    new_hulls = combine_hulls_on_same_level(contours,threshold=20)\n",
        "    # Scribble Generation\n",
        "    predictedScribbles=[]\n",
        "    for hull in new_hulls:\n",
        "        hull = np.asarray(hull,dtype=np.int32).reshape((-1,2)).tolist()\n",
        "        scr_ = generateScribble(H,W,hull)\n",
        "        if scr_ is not None:\n",
        "            predictedScribbles.append(scr_)\n",
        "    return predictedScribbles\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gHrZ6Q85Q3N"
      },
      "source": [
        "# User Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV1vSAKi5DaY",
        "outputId": "8a0be1cc-7b2d-4898-8095-a70c0627b9f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide the full path to the folder : /content/\n",
            " Choose the model checkpoint (BKS / I2 ) : BKS\n",
            "Please provide the full path to the output folder : BKS_OPT\n",
            "BKS.pt is already existing .. Skipping download !\n"
          ]
        }
      ],
      "source": [
        "# Please enter valid values\n",
        "IMAGE_FOLDER = input(\"Please provide the full path to the folder : \")\n",
        "MODEL_CHECKPOINT_TYPE = input(\" Choose the model checkpoint (BKS / I2 ) : \")\n",
        "OUTPUT_FOLDER = input(\"Please provide the full path to the output folder : \")\n",
        "downloadWeights(modelType=MODEL_CHECKPOINT_TYPE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yegOTgNVlDIu"
      },
      "source": [
        "# Visual Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVAy65-1lg4y",
        "outputId": "34db76d6-6e87-4049-e5ef-55c19b96943b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scribble Output : \n"
          ]
        }
      ],
      "source": [
        "def visualInference(folderPath,outputPath):\n",
        "\n",
        "  # Output Folder Creation\n",
        "  os.makedirs(outputPath,exist_ok = True)\n",
        "  os.makedirs(os.path.join(outputPath,'binaryImages'),exist_ok = True)\n",
        "  os.makedirs(os.path.join(outputPath,'scribbleImages'),exist_ok = True)\n",
        "\n",
        "  if os.path.exists(folderPath):\n",
        "    fileNames =  [filename for filename in os.listdir(folderPath) if filename.endswith('.jpg') or filename.endswith('.png')]\n",
        "  # Iterating through the samples and generating the results\n",
        "  for f in fileNames :\n",
        "    path = os.path.join(IMAGE_FOLDER,f)\n",
        "    img = cv2.imread(path)\n",
        "    binaryOutput,scribbleOutput,res = inferenceNetwork(network,path,PDIM=256,DIM=256,OVERLAP=0.50,THRESHOLD=0.5,save=True)\n",
        "    print('Scribble Output : ')\n",
        "    cv2_imshow(scribbleOutput)\n",
        "    print('Binary Output : ')\n",
        "    cv2_imshow(binaryOutput)\n",
        "    # Scribbles\n",
        "    binaryOutput=np.uint8(binaryOutput)\n",
        "    scribbleOutput=np.uint8(scribbleOutput)\n",
        "    scribbles = postProcess(scribbleOutput,binaryOutput,binaryThreshold=50,rectangularKernel=50)\n",
        "    img2 = copy.deepcopy(img)\n",
        "    for p in scribbles:\n",
        "        p = np.asarray(p,dtype=np.int32).reshape((-1,1,2))\n",
        "        img2 = cv2.polylines(img2, [p],False, (0,255,0),2)\n",
        "\n",
        "\n",
        "    # Writing to the directory ..\n",
        "    cv2.imwrite(os.path.join(outputPath,'binaryImages/{}'.format(f)),binaryOutput)\n",
        "    cv2.imwrite(os.path.join(outputPath,'scribbleImages/{}'.format(f)),scribbleOutput)\n",
        "\n",
        "    print('Image with Scribbles Overlaid :')\n",
        "    cv2_imshow(img2)\n",
        "\n",
        "# Network Weights Loading..\n",
        "network.load_state_dict(torch.load(MODEL_CHECKPOINT_TYPE+'.pt',map_location=device),strict=True)\n",
        "visualInference(IMAGE_FOLDER,OUTPUT_FOLDER)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
